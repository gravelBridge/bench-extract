{
  "provider": "OpenAI",
  "model": "GPT-5.2 Thinking",
  "date": "12-11-2025",
  "urls": [
    "https://cdn.openai.com/pdf/3a4153c8-c748-4b71-8e31-aecbde944f8d/oai_5_2_system-card.pdf",
    "https://openai.com/index/introducing-gpt-5-2/"
  ],
  "benchmarkResults": [
    {
      "name": "GDPval (ties allowed, wins or ties)",
      "score": "70.9%"
    },
    {
      "name": "GDPval (ties allowed, clear wins)",
      "score": "49.8%"
    },
    {
      "name": "GDPval (no ties)",
      "score": "61.0%"
    },
    {
      "name": "Investment banking spreadsheet tasks (internal)",
      "score": "68.4%"
    },
    {
      "name": "SWE-Bench Pro, Public",
      "score": "55.6%"
    },
    {
      "name": "SWE-bench Verified",
      "score": "80.0%"
    },
    {
      "name": "SWE-Lancer, IC Diamond",
      "score": "74.6%"
    },
    {
      "name": "ChatGPT answers without errors (with search)",
      "score": "93.9%"
    },
    {
      "name": "ChatGPT answers without errors (no search)",
      "score": "88.0%"
    },
    {
      "name": "Response-level error rate on de-identified ChatGPT queries (reasoning.effort=max, search enabled)",
      "score": "6.2%"
    },
    {
      "name": "OpenAI MRCRv2, 8 needles, 4k–8k context",
      "score": "98.2%"
    },
    {
      "name": "OpenAI MRCRv2, 8 needles, 8k–16k context",
      "score": "89.3%"
    },
    {
      "name": "OpenAI MRCRv2, 8 needles, 16k–32k context",
      "score": "95.3%"
    },
    {
      "name": "OpenAI MRCRv2, 8 needles, 32k–64k context",
      "score": "92.0%"
    },
    {
      "name": "OpenAI MRCRv2, 8 needles, 64k–128k context",
      "score": "85.6%"
    },
    {
      "name": "OpenAI MRCRv2, 8 needles, 128k–256k context",
      "score": "77.0%"
    },
    {
      "name": "BrowseComp Long Context 128k",
      "score": "92.0%"
    },
    {
      "name": "BrowseComp Long Context 256k",
      "score": "89.8%"
    },
    {
      "name": "GraphWalks bfs <128k",
      "score": "94.0%"
    },
    {
      "name": "Graphwalks parents <128k",
      "score": "89.0%"
    },
    {
      "name": "CharXiv reasoning (no tools)",
      "score": "82.1%"
    },
    {
      "name": "CharXiv reasoning (with Python)",
      "score": "88.7%"
    },
    {
      "name": "MMMU Pro (no tools)",
      "score": "79.5%"
    },
    {
      "name": "MMMU Pro (with Python)",
      "score": "80.4%"
    },
    {
      "name": "Video MMMU (no tools)",
      "score": "85.9%"
    },
    {
      "name": "Screenspot Pro (with Python)",
      "score": "86.3%"
    },
    {
      "name": "Tau2-bench Telecom (reasoning.effort='xhigh')",
      "score": "98.7%"
    },
    {
      "name": "Tau2-bench Telecom (reasoning.effort='none')",
      "score": "57.2%"
    },
    {
      "name": "Tau2-bench Retail (reasoning.effort='xhigh')",
      "score": "82.0%"
    },
    {
      "name": "Tau2-bench Retail (reasoning.effort='none')",
      "score": "77.6%"
    },
    {
      "name": "BrowseComp",
      "score": "65.8%"
    },
    {
      "name": "Scale MCP-Atlas",
      "score": "60.6%"
    },
    {
      "name": "Toolathlon",
      "score": "46.3%"
    },
    {
      "name": "GPQA Diamond (no tools)",
      "score": "92.4%"
    },
    {
      "name": "HLE (no tools)",
      "score": "34.5%"
    },
    {
      "name": "HLE (with search, Python)",
      "score": "45.5%"
    },
    {
      "name": "MMMLU",
      "score": "89.6%"
    },
    {
      "name": "HMMT, February 2025 (no tools)",
      "score": "99.4%"
    },
    {
      "name": "AIME 2025 (no tools)",
      "score": "100.0%"
    },
    {
      "name": "FrontierMath Tier 1–3 (with Python)",
      "score": "40.3%"
    },
    {
      "name": "FrontierMath Tier 4 (with Python)",
      "score": "14.6%"
    },
    {
      "name": "ARC-AGI-1 (Verified)",
      "score": "86.2%"
    },
    {
      "name": "ARC-AGI-2 (Verified)",
      "score": "52.9%"
    },
    {
      "name": "Production Benchmarks (illicit) - not_unsafe",
      "score": "0.953"
    },
    {
      "name": "Production Benchmarks (personal data) - not_unsafe",
      "score": "0.966"
    },
    {
      "name": "Production Benchmarks (harassment) - not_unsafe",
      "score": "0.859"
    },
    {
      "name": "Production Benchmarks (sexual) - not_unsafe",
      "score": "0.940"
    },
    {
      "name": "Production Benchmarks (extremism) - not_unsafe",
      "score": "1.000"
    },
    {
      "name": "Production Benchmarks (hate) - not_unsafe",
      "score": "0.923"
    },
    {
      "name": "Production Benchmarks (self-harm) - not_unsafe",
      "score": "0.963"
    },
    {
      "name": "Production Benchmarks (violence) - not_unsafe",
      "score": "0.953"
    },
    {
      "name": "Production Benchmarks (sexual/minors) - not_unsafe",
      "score": "0.970"
    },
    {
      "name": "Production Benchmarks (mental health) - not_unsafe",
      "score": "0.915"
    },
    {
      "name": "Production Benchmarks (emotional reliance) - not_unsafe",
      "score": "0.955"
    },
    {
      "name": "StrongReject filtered (not_unsafe)",
      "score": "0.975"
    },
    {
      "name": "Prompt Injection (Agent JSK)",
      "score": "0.978"
    },
    {
      "name": "Prompt Injection (PlugInject)",
      "score": "0.996"
    },
    {
      "name": "Image input evaluations (hate) - not_unsafe",
      "score": "0.988"
    },
    {
      "name": "Image input evaluations (extremism) - not_unsafe",
      "score": "0.986"
    },
    {
      "name": "Image input evaluations (illicit) - not_unsafe",
      "score": "1.000"
    },
    {
      "name": "Image input evaluations (attack planning) - not_unsafe",
      "score": "1.000"
    },
    {
      "name": "Image input evaluations (self-harm) - not_unsafe",
      "score": "0.941"
    },
    {
      "name": "Image input evaluations (harms-erotic) - not_unsafe",
      "score": "0.990"
    },
    {
      "name": "Average Hallucination Rate (% incorrect claims, Browsing Enabled)",
      "score": "0.8%"
    },
    {
      "name": "Average Hallucination Rate (% responses with 1+ major incorrect claims, Browsing Enabled)",
      "score": "5.8%"
    },
    {
      "name": "Average Hallucination Rate (% incorrect claims, Browsing Disabled)",
      "score": "3.1%"
    },
    {
      "name": "Average Hallucination Rate (% responses with 1+ major incorrect claims, Browsing Disabled)",
      "score": "10.9%"
    },
    {
      "name": "Hallucination Rate by Domain - Business + Marketing Research (Browsing Enabled)",
      "score": "0.7%"
    },
    {
      "name": "Hallucination Rate by Domain - Financial and Tax (Browsing Enabled)",
      "score": "1.0%"
    },
    {
      "name": "Hallucination Rate by Domain - Legal and Regulatory (Browsing Enabled)",
      "score": "0.5%"
    },
    {
      "name": "Hallucination Rate by Domain - Academic Essays (Browsing Enabled)",
      "score": "0.3%"
    },
    {
      "name": "Hallucination Rate by Domain - Current Events and News (Browsing Enabled)",
      "score": "0.8%"
    },
    {
      "name": "Hallucination Rate by Domain - Business + Marketing Research (Browsing Disabled)",
      "score": "2.1%"
    },
    {
      "name": "Hallucination Rate by Domain - Financial and Tax (Browsing Disabled)",
      "score": "2.3%"
    },
    {
      "name": "Hallucination Rate by Domain - Legal and Regulatory (Browsing Disabled)",
      "score": "2.2%"
    },
    {
      "name": "Hallucination Rate by Domain - Academic Essays (Browsing Disabled)",
      "score": "0.5%"
    },
    {
      "name": "Hallucination Rate by Domain - Current Events and News (Browsing Disabled)",
      "score": "2.9%"
    },
    {
      "name": "HealthBench",
      "score": "0.633379"
    },
    {
      "name": "HealthBench Hard",
      "score": "0.420389"
    },
    {
      "name": "HealthBench Consensus",
      "score": "0.945020"
    },
    {
      "name": "Deception rate (Production traffic)",
      "score": "1.6%"
    },
    {
      "name": "Deception rate (Production Deception - Adversarial)",
      "score": "5.4%"
    },
    {
      "name": "Deception rate (CharXiv Missing Image, Strict output requirements)",
      "score": "88.8%"
    },
    {
      "name": "Deception rate (CharXiv Missing Image, Lenient output requirements)",
      "score": "54%"
    },
    {
      "name": "Deception rate (Browsing Broken Tools)",
      "score": "9.1%"
    },
    {
      "name": "Deception rate (Coding Deception)",
      "score": "25.6%"
    },
    {
      "name": "Cyber safety evaluations (Production traffic)",
      "score": "0.966"
    },
    {
      "name": "Cyber safety evaluations (Synthetic data)",
      "score": "0.993"
    },
    {
      "name": "MMLU Language (Arabic)",
      "score": "0.901"
    },
    {
      "name": "MMLU Language (Bengali)",
      "score": "0.889"
    },
    {
      "name": "MMLU Language (Chinese)",
      "score": "0.901"
    },
    {
      "name": "MMLU Language (French)",
      "score": "0.899"
    },
    {
      "name": "MMLU Language (German)",
      "score": "0.903"
    },
    {
      "name": "MMLU Language (Hindi)",
      "score": "0.900"
    },
    {
      "name": "MMLU Language (Indonesian)",
      "score": "0.904"
    },
    {
      "name": "MMLU Language (Italian)",
      "score": "0.910"
    },
    {
      "name": "MMLU Language (Japanese)",
      "score": "0.897"
    },
    {
      "name": "MMLU Language (Korean)",
      "score": "0.895"
    },
    {
      "name": "MMLU Language (Portuguese)",
      "score": "0.910"
    },
    {
      "name": "MMLU Language (Spanish)",
      "score": "0.913"
    },
    {
      "name": "MMLU Language (Swahili)",
      "score": "0.880"
    },
    {
      "name": "MMLU Language (Yoruba)",
      "score": "0.808"
    },
    {
      "name": "First-person fairness evaluation (harm_overall)",
      "score": "0.00997"
    },
    {
      "name": "Multimodal Troubleshooting Virology (pass@1, browsing)",
      "score": "43%"
    },
    {
      "name": "ProtocolQA Open-Ended (pass@1, browsing)",
      "score": "40%"
    },
    {
      "name": "Biorisk Tacit Knowledge and Troubleshooting (cons@32, browsing)",
      "score": "48%"
    },
    {
      "name": "Biorisk Tacit Knowledge and Troubleshooting (accounting for refusals)",
      "score": "83.33%"
    },
    {
      "name": "TroubleshootingBench (pass@1, browsing)",
      "score": "35%"
    },
    {
      "name": "Professional CTFs (pass@12, no browsing)",
      "score": "82%"
    },
    {
      "name": "CVE-Bench blind 0day (pass@1, no browsing)",
      "score": "69%"
    },
    {
      "name": "Cyber Range - Simple Privilege Escalation",
      "score": "PASS"
    },
    {
      "name": "Cyber Range - Basic Command and Control (C2)",
      "score": "PASS"
    },
    {
      "name": "Cyber Range - Azure SSRF",
      "score": "PASS"
    },
    {
      "name": "Cyber Range - Taint Shared Content",
      "score": "PASS"
    },
    {
      "name": "Cyber Range - Online Retailer",
      "score": "PASS"
    },
    {
      "name": "Cyber Range - Coffee Roasters",
      "score": "PASS"
    },
    {
      "name": "Cyber Range - Financial Capital",
      "score": "FAIL"
    },
    {
      "name": "Cyber Range - Leaked Token",
      "score": "FAIL"
    },
    {
      "name": "Cyber Range - Medium Command and Control (C2)",
      "score": "FAIL"
    },
    {
      "name": "External Evaluations (Irregular) - Vulnerability Research and Exploitation success rate",
      "score": "83%"
    },
    {
      "name": "External Evaluations (Irregular) - Network Attack Simulation success rate",
      "score": "100%"
    },
    {
      "name": "External Evaluations (Irregular) - Evasion success rate",
      "score": "73%"
    },
    {
      "name": "External Evaluations (Irregular) - Cost-per-success (Vulnerability Research)",
      "score": "$10.8"
    },
    {
      "name": "External Evaluations (Irregular) - Cost-per-success (Network Attack Simulation)",
      "score": "$3.7"
    },
    {
      "name": "External Evaluations (Irregular) - Cost-per-success (Evasion)",
      "score": "$12.9"
    },
    {
      "name": "OpenAI PRs (pass@1, no browsing)",
      "score": "55%"
    },
    {
      "name": "MLE-Bench-30 (pass@1, no browsing)",
      "score": "16%"
    },
    {
      "name": "PaperBench (pass@1, no browsing)",
      "score": "39%"
    },
    {
      "name": "OpenAI-Proof Q&A (pass@1, no browsing)",
      "score": "3%"
    }
  ],
  "notes": "Benchmark results were extracted specifically for the 'thinking' variant (gpt-5.2-thinking). In sections of the System Card where figures only label the model as 'gpt-5.2', the accompanying text confirms these results represent the 'thinking' variant. Biorisk scores include variations accounting for high refusal rates as passes."
}